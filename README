HiTune-hadoop1.x

This is the README for HiTune-hadoop1.x. For any futher questions, please contact jason.dai@intel.com or shengsheng.huang@intel.com.

============================================================================
1. Overview of HiTune-hadoop1.x

HiTune is a Hadoop performance analyzer. It consists of three major components as follows:
    1) Tracker - lightweight agents running on each node in the Hadoop cluster to collect runtime information, including sysstat, Hadoop metrics and job history, and Java instrumentations.
    2) Aggregation Engine - distributed framework that merges the results of the trackers, which is currently implemented using Chukwa.
    3) Analysis Engine - the program that conducts the performance analysis and generates the analysis report, which is implemented as a series of Hadoop jobs.

In addition, a sample utility is provided, which allows the analysis report to be visualized using Excel 2007.

============================================================================
2. Main working flow of hitune

1. chukwa agent on each hadoop nodes collects hadoop running information and sends it to a random chukwa collector.
2. chukwa collector collects the information received from agents and stores them in HDFS.
3. chukwa processor reads the raw data in HDFS and then orders, refines, formats and store the formatted data in HDFS.
4. hitune gets the information of hadoop history log and store the useful information in HDFS.
5. run the hitune analysis script, hitune analyses the data in HDFS and generates the csv files.
5. copy the csv files from HDFS to local and use visualreport/AnalysisReport.xlsm to analyse them. 

============================================================================
3. Quick Start

Please follow the steps below for install and use HiTune.
Hitune is a analysis tool used to analyse hadoop performance, so we need a hadoop cluster for hitune to analyse. Besides, hitune will generate map/reduce jobs when doing the analysis job, in another word, we need a hadoop cluster to run hitune. You can use two hadoop clusters to do the different parts or just use one cluster to do the two parts. (For convenience, we only discuss the later situation)

3.1 Prerequisites 
    1) Set up a separate cluster running Hadoop and HDFS. (In the minimum setup, the cluster can comprise just a single node.)
    2) Time-synchronize all the nodes in the Hadoop cluster and the Chukwa cluster (e.g., using NTP and crontab).
    3) Install sysstat (version 9.0.x) on each node in the Hadoop cluster (The built-in systat package v7.x.x in Redhat is not compatible with current HiTune implementation).
    4) Disable Hadoop JVM reuse, which is currently not supported by HiTune.
		modify $HADOOP_HOME/conf/mapred-site.xml, add:
			<property>
				<name>mapred.job.reuse.jvm.num.tasks</name>
				<value>1</value>
			</property>
    
    Please be noted that HiTune-hadoop1.x has been tested on Red Hat Enterprise Linux; it is also known to run successfully on Ubuntu Server. It is not tested to run on other Linux distributions.

3.2 Installation
    1)  Extract the HiTune package to a local folder (such as HiTune-hadoop1.x).
    2)  Prepare a configuration file through by running:
            $>cd HiTune-hadoop1.x
            $>./configure
        -------------------------------------------------
        This will allow the user to input the values of various parameters - e.g., the role of the cluster ("chukwa" or "hadoop"), where to install the HiTune package ($INSTALL_DIR) and where Hadoop is installed ($HADOOP_HOME) - and then generate either chukwa-cluster.conf or hadoop-cluster.conf file under the folder hitune-0.9.

        Alternatively,
        You can manually prepare the configuration file according to the ".conf" file provided in the package, then and run 
            $>./configure -f configure_file
    3)  Install hitune package on one node in your Chukwa or Hadoop cluster by first copying the hitune folder to the node, and then running:
            On Chukwa cluster:
                $>./install -f chukwa-cluster.conf
            On Hadoop cluster:
                $>./install -f hadoop-cluster.conf

        OR, if the configurations for both clusters are the same, you can reuse the configuration file by running:
            On Chukwa cluster:
                $>./install -f configure_file -r chukwa
            On Hadoop cluster:
                $>./install -f configure_file -r hadoop

        [IMPORTANT] Repeat this step on every other node in the Chukwa or Hadoop cluster; alternatively, you can just sync the $INSTALL_DIR and $HADOOP_HOME on this node to every other node in the Chukwa or Hadoop cluster.


3.3 Use HiTune
    1) Before using HiTune, make sure the Hadoop framework in the Chukwa cluster is started.
    2) Start the Chukwa agents, collectors and processor. One way to do that is to run the $INSTALL_DIR/sbin/start-chukwa.sh script on the Chukwa processor node, which requires
          * Enable password-less SSH from the Chukwa processor node to each node in the Hadoop cluster and each node in the Chukwa cluster.
    3) Submit the Hadoop job to the Hadoop cluster; to enable Java instrumentation, modify $HADOOP_HOME/conf/mapred-site.xml, add:  
			<property>
				<name>mapred.child.java.opts</name>
				<value>-Xmx200m -javaagent:[your INSTALL_DIR path ]/hitune/HiTuneInstrumentAgent-0.9.jar=traceoutput=[your INSTALL_DIR path ]/hitune_output,taskid=@taskid@
				</value>
			</property>
    4) After the job finishes, the Chukwa agents may be optionally stopped by running the 
		$INSTALL_DIR/sbin/stop-agents.sh script on the Chukwa processor node
	5). run a hadoop job, remember the job id, eg. job_XXXXXXXXXXXX_XXXX, wait a while and check the ./JOBS in HDFS. Hitune will process and generate the file in ./JOBS every five minutes in your set up successfully. 
	6). $INSTALL_DIR/hitune/bin/HiTuneAnalysis.sh -id XXXXXXXXXXXX_XXXX, hitune will generate the csv files in HDFS, download them to local file system
	7). download the directory $INSTALL_DIR/hitune/visualreport to local system.
	8). copy visualreport/TYPE to the csv files directory you downloaded and open visualreport/AnalysisReport.xlsm. Load the csv files and do the analyse you want.
    9) The analysis output is stored in the HDFS of the Chukwa cluster, and its location is specified in the $INSTALL_DIR/hitune/conf/report/conf.xml file.

3.4 Details about visualizing the Report
    1) Copy the generated output folder to a Windows machine with Excel 2007 or above.
    2) Create a file named as "TYPE" under the output folder according to $INSTALL_DIR/hitune/visualreport/TYPE.
    3) Open $INSTALL_DIR/hitune/visualreport/AnalysisReport.xlsm and load the output folder. 

3.5 Check HiTune runtime status
    1) To enable full functionalities of the HiTune status checker tool, install the "expect" tool on the Chukwa processor node.
    2) Edit $INSTALL_DIR/hitune/conf/HiTuneStatusCheck-env.sh file on the Chukwa processor node
    3) On the Chukwa processor node, check if the Chukwa cluster and HiTune is healthy by running:
               $> $INSTALL_DIR/hitune/bin/HiTuneStatusCheck.sh
       The final status report is stored as /tmp/HiTuneStatusCheck.report, more details about the report can be found by adding "-h" argument in command line
    
============================================================================
4. Details

The detailed guide for deploying, configuring and managing Chukwa can be found in the official documentation for Chukwa 0.6.0

In case of some IO exception in HDFS of the chukwa-cluster, to configure "dfs.datanode.max.xcievers" of DataNode to be higher than the default value 256, e.g., 65536. 

The HiTune instrument outputs are stored under the $INSTALL_DIR/hitune_output folder on each node in the Hadoop cluster; it will grow over time and will need to be periodically pruned.

The analysis configuration files for hadoop job are automatically created under /.JOBS/ on HDFS of chukwa cluster; the file number will be increased by the finished hadoop job number, so it needs to archive and prune that folder periodically.

More details of the sample utility for visualizing the analysis report can be found in the $INSTALL_DIR/hitune/visualreport/HiTune_visual_report_user_manual.pdf file. One visual report examples are provided in $INSTALL_DIR/hitune/visualreport/visual_report_example.

============================================================================
5. Trouble shooting

5.1 the io, cpu, mem status info cannot be collected or can not find /.JOBS folder in HDFS:
	1)try delete the ¡°$2¡± in $CHUKWA_HOME/conf/initial_adaptor on each agent node and restart the agents.
	2)try to find the position of the instructions "sar", "iostat" and modify them in etc/chukwa/initial_adaptors

5.2 cannot analyse hadoop metrics2 when using HiTuneAnalysis:
	verify the conf.xml files in$CHUKWA_HOME/hitune/conf/report/hadoopmetrics/* according to /chukwa/repos in HDFS

5.3 HDFS space is full:
	The following directories will grow over time and will need to be periodically pruned:(e.g)
	/chukwa/repos
	/chukwa/finalArchives
	Its location is specified in the $INSTALL_DIR/hitune/conf/report/conf.xml file.